{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [BERT](https://huggingface.co/docs/transformers/model_doc/bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`float16` -> baisse la précision pour accélérer les calculs<br>\n",
    "`sdpa` -> Scaled Dot Product Attention [documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "from torch import float16\n",
    "\n",
    "bert_model_name : str = \"bert-base-uncased\"\n",
    "\n",
    "# model = BertModel.from_pretrained(bert_model_name, \n",
    "#     torch_dtype=float16, attn_implementation=\"sdpa\"\n",
    "# )\n",
    "#First time launching : 2m10.5s\n",
    "# model.save_pretrained(\"../models/2025-03-04-bert-base-uncased\", from_pt = True)\n",
    "model = BertModel.from_pretrained(\"../models/2025-03-04-bert-base-uncased\",\n",
    "            torch_dtype = float16, attn_implementation = \"sdpa\")\n",
    "# second launch : 2.0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Fine-tuning un classifieur multi-label](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb#scrollTo=4wxY3x-ZZz8h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of tweets, labeled with one or more emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_bert_model_name = \"../models/2025-03-04-bert-base-uncased\"\n",
    "model_name = \"bert-base-uncased\" # FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\n",
    "# 10s dont forget to press y + 'Enter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ID', 'Tweet', 'anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust'],\n",
       "        num_rows: 6838\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ID', 'Tweet', 'anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust'],\n",
       "        num_rows: 3259\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ID', 'Tweet', 'anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust'],\n",
       "        num_rows: 886\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pessimism</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-En-21441</td>\n",
       "      <td>“Worry is a down payment on a problem you may ...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-En-31535</td>\n",
       "      <td>Whatever you decide to do make sure it makes y...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-En-21068</td>\n",
       "      <td>@Max_Kellerman  it also helps that the majorit...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-En-31436</td>\n",
       "      <td>Accept the challenges so that you can literall...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-En-22195</td>\n",
       "      <td>My roommate: it's okay that we can't spell bec...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6833</th>\n",
       "      <td>2017-En-21383</td>\n",
       "      <td>@nicky57672 Hi! We are working towards your hi...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6834</th>\n",
       "      <td>2017-En-41441</td>\n",
       "      <td>@andreamitchell said @berniesanders not only d...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6835</th>\n",
       "      <td>2017-En-10886</td>\n",
       "      <td>@isthataspider @dhodgs i will fight this guy! ...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6836</th>\n",
       "      <td>2017-En-40662</td>\n",
       "      <td>i wonder how a guy can broke his penis while h...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6837</th>\n",
       "      <td>2017-En-31003</td>\n",
       "      <td>I'm highly animated even though I'm decomposing.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6838 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID                                              Tweet  anger  \\\n",
       "0     2017-En-21441  “Worry is a down payment on a problem you may ...  False   \n",
       "1     2017-En-31535  Whatever you decide to do make sure it makes y...  False   \n",
       "2     2017-En-21068  @Max_Kellerman  it also helps that the majorit...   True   \n",
       "3     2017-En-31436  Accept the challenges so that you can literall...  False   \n",
       "4     2017-En-22195  My roommate: it's okay that we can't spell bec...   True   \n",
       "...             ...                                                ...    ...   \n",
       "6833  2017-En-21383  @nicky57672 Hi! We are working towards your hi...  False   \n",
       "6834  2017-En-41441  @andreamitchell said @berniesanders not only d...  False   \n",
       "6835  2017-En-10886  @isthataspider @dhodgs i will fight this guy! ...   True   \n",
       "6836  2017-En-40662  i wonder how a guy can broke his penis while h...  False   \n",
       "6837  2017-En-31003   I'm highly animated even though I'm decomposing.  False   \n",
       "\n",
       "      anticipation  disgust   fear    joy   love  optimism  pessimism  \\\n",
       "0             True    False  False  False  False      True      False   \n",
       "1            False    False  False   True   True      True      False   \n",
       "2            False     True  False   True  False      True      False   \n",
       "3            False    False  False   True  False      True      False   \n",
       "4            False     True  False  False  False     False      False   \n",
       "...            ...      ...    ...    ...    ...       ...        ...   \n",
       "6833         False    False  False  False  False     False      False   \n",
       "6834          True    False  False  False  False     False      False   \n",
       "6835         False     True  False  False  False     False       True   \n",
       "6836         False    False  False  False  False     False      False   \n",
       "6837         False    False  False  False  False     False       True   \n",
       "\n",
       "      sadness  surprise  trust  \n",
       "0       False     False   True  \n",
       "1       False     False  False  \n",
       "2       False     False  False  \n",
       "3       False     False  False  \n",
       "4       False     False  False  \n",
       "...       ...       ...    ...  \n",
       "6833    False     False  False  \n",
       "6834    False      True  False  \n",
       "6835    False     False  False  \n",
       "6836    False      True  False  \n",
       "6837    False     False  False  \n",
       "\n",
       "[6838 rows x 13 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [label for label in dataset['train'].features.keys() if label not in ['ID', 'Tweet']]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utilise un tokeniser par défaut (`AutoTokenizer` from `transformer`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "from datasets.formatting.formatting import LazyBatch\n",
    "\n",
    "def preprocess_data(batch_of_rows : LazyBatch) -> BatchEncoding:\n",
    "    # Takes in a batch of rows (as a : LazyBatch ~ dataframe ish) \n",
    "    \n",
    "    # collect the text and tokenize it \n",
    "    text = batch_of_rows[\"Tweet\"]\n",
    "    encoding : BatchEncoding = tokenizer(\n",
    "        text, padding = \"max_length\", truncation = True, max_length = 128 \n",
    "    )\n",
    "    # Create a mattrix collecting all the metadata (emotions associated to the \n",
    "    # tweet)\n",
    "    labels_matrix = np.zeros((len(text), len(labels)))\n",
    "    for label in label2id:\n",
    "        labels_matrix[:,label2id[label]] = batch_of_rows[label]\n",
    "\n",
    "    # Associate the metadata to the encodings\n",
    "    encoding[\"labels\"] = labels_matrix.tolist()\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le format `BatchEncoding` permet d'utiliser un modèle notamment grace au label `input_ids` et formatte les entrées selon ce qui est attendu (`[CLS] / [SEP] / ...`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(preprocess_data,\n",
    "    batched = True, remove_columns = dataset[\"train\"].column_names\n",
    ")\n",
    "# Comme on utilise pytorch on met nos données au format torch\n",
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "print(encoded_dataset['train'][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original input :\n",
      " “Worry is a down payment on a problem you may never have'.  Joyce Meyer.  #motivation #leadership #worry\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Input tokenized :\n",
      " tensor([  101,  1523,  4737,  2003,  1037,  2091,  7909,  2006,  1037,  3291,\n",
      "         2017,  2089,  2196,  2031,  1005,  1012, 11830, 11527,  1012,  1001,\n",
      "        14354,  1001,  4105,  1001,  4737,   102,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Input decoded :\n",
      " [CLS] “ worry is a down payment on a problem you may never have '. joyce meyer. # motivation # leadership # worry [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original input :\\n\",dataset['train'][0][\"Tweet\"])\n",
    "print('- '*50)\n",
    "print(\"Input tokenized :\\n\",encoded_dataset['train'][0][\"input_ids\"])\n",
    "print('- '*50)\n",
    "print(\"Input decoded :\\n\",tokenizer.decode(encoded_dataset['train'][0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.])\n",
      "=>  ['anticipation', 'optimism', 'trust']\n"
     ]
    }
   ],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "def get_labels(example_labels : Tensor) -> list[str]:\n",
    "    return [id2label[idx] \n",
    "            for idx,label in enumerate(example_labels) if label == 1]\n",
    "\n",
    "print(encoded_dataset[\"train\"][0][\"labels\"]) # est une matrice !!!\n",
    "print(\"=> \",get_labels(encoded_dataset[\"train\"][0][\"labels\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifieur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On load un classifieur (BERT + couche linéaire initialement aléatoire) que l'on va entraîner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../models/2025-03-04-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "encoder_classifier = AutoModelForSequenceClassification.from_pretrained(\n",
    "        custom_bert_model_name,\n",
    "        problem_type = \"multi_label_classification\", num_labels = len(labels),\n",
    "        id2label = id2label, label2id = label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'alerte : \n",
    "```\n",
    "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../models/2025-03-04-bert-base-uncased/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "```\n",
    "indique bien que nous n'avons récupéré QUE le modèle de plongement (\"bert\"), mais que la couche linéaire elle n'existait pas et a donc été intialisée aléatoirement. Nous sommes donc vivement invités à entraîner *au moins* le classifieur (`['classifier.bias', 'classifier.weight']`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec l'API huggingface `Trainer`. On doit alors créer 2 objets : \n",
    "- `TrainingArguments`\n",
    "- `Trainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "batch_size = 8\n",
    "metric_name = \"f1\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"../models/2025-03-04-classifieur_entraine\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de métrique d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "from torch.nn import Sigmoid\n",
    "\n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/ \n",
    "def multi_label_metrics(results_matrix, labels : Tensor, threshold : float = 0.5\n",
    "                        ) -> dict:\n",
    "    '''Taking a results matrix (batch_size x num_labels), the function (with a \n",
    "    threshold) associates labels to the results => y_pred\n",
    "    From this y_pred matrix, evaluate the f1_micro, roc_auc and accuracy metrics\n",
    "    '''\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = Sigmoid()\n",
    "    probs = sigmoid(Tensor(results_matrix))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    return {'f1': f1_micro_average,\n",
    "             'roc_auc': roc_auc,\n",
    "             'accuracy': accuracy}\n",
    "\n",
    "def compute_metrics(model_output: EvalPrediction):\n",
    "    if isinstance(model_output.predictions,tuple):\n",
    "        results_matrix = model_output.predictions[0]\n",
    "    else:\n",
    "        results_matrix = model_output.predictions\n",
    "\n",
    "    metrics = multi_label_metrics(results_matrix=results_matrix, \n",
    "        labels=model_output.label_ids)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemple de `forward pass`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp_ex_input_ids : (torch.LongTensor)\n",
      " tensor([  101,  1523,  4737,  2003,  1037,  2091,  7909,  2006,  1037,  3291,\n",
      "         2017,  2089,  2196,  2031,  1005,  1012, 11830, 11527,  1012,  1001,\n",
      "        14354,  1001,  4105,  1001,  4737,   102,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  \n",
      "fp_ex_input_ids.unsqueeze(0) : (torch.LongTensor)\n",
      " tensor([[  101,  1523,  4737,  2003,  1037,  2091,  7909,  2006,  1037,  3291,\n",
      "          2017,  2089,  2196,  2031,  1005,  1012, 11830, 11527,  1012,  1001,\n",
      "         14354,  1001,  4105,  1001,  4737,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "fp_ex_input_labels : (torch.FloatTensor)\n",
      " tensor([0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.])\n",
      "-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  \n",
      "fp_ex_input_labels.unsqueeze(0) : (torch.FloatTensor)\n",
      " tensor([[0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.]])\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "0.90 s — outputs : <class 'transformers.modeling_outputs.SequenceClassifierOutput'>\n",
      " SequenceClassifierOutput(loss=tensor(0.7428, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), logits=tensor([[-0.2304,  0.4649,  0.0580, -0.3462, -0.2489,  1.0427, -0.0357,  0.0151,\n",
      "          0.0627, -0.1961, -0.8179]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "fp_ex_input_ids = encoded_dataset['train']['input_ids'][0]\n",
    "fp_ex_input_labels = encoded_dataset['train']['labels'][0]\n",
    "print(f\"fp_ex_input_ids : ({fp_ex_input_ids.type()})\\n\",fp_ex_input_ids)\n",
    "print('-  ' * 20)\n",
    "print(f\"fp_ex_input_ids.unsqueeze(0) : ({fp_ex_input_ids.unsqueeze(0).type()})\\n\",fp_ex_input_ids.unsqueeze(0))\n",
    "print('- ' * 30)\n",
    "print(f\"fp_ex_input_labels : ({fp_ex_input_labels.type()})\\n\",fp_ex_input_labels)\n",
    "print('-  ' * 20)\n",
    "print(f\"fp_ex_input_labels.unsqueeze(0) : ({fp_ex_input_labels.unsqueeze(0).type()})\\n\",fp_ex_input_labels.unsqueeze(0))\n",
    "print('- ' * 30)\n",
    "\n",
    "# NOTE Je sais pas bien ce que ce à quoi ce \"unsqueeze\" sert\n",
    "from time import time\n",
    "\n",
    "t1 = time()\n",
    "outputs = encoder_classifier(\n",
    "    input_ids=fp_ex_input_ids.unsqueeze(0), \n",
    "    labels=fp_ex_input_labels.unsqueeze(0))\n",
    "print(f\"{time() - t1:.2f} s — outputs : {type(outputs)}\\n\",outputs)\n",
    "\n",
    "del fp_ex_input_ids, fp_ex_input_labels, outputs, t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xs/d90v1vkn16db_7z493h6kwt40000gn/T/ipykernel_75718/1560193853.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(encodeur_classifier, training_args,\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(encoder_classifier, training_args,\n",
    "                  train_dataset = encoded_dataset[\"train\"].select(range(0,20)),\n",
    "                  eval_dataset = encoded_dataset[\"validation\"].select(range(0,10)),\n",
    "                  tokenizer = tokenizer,\n",
    "                  compute_metrics = compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.710008</td>\n",
       "      <td>0.311688</td>\n",
       "      <td>0.538523</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.672419</td>\n",
       "      <td>0.327869</td>\n",
       "      <td>0.569556</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.633361</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.572499</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.609141</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.541734</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.601324</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.547352</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.884125 s to train\n"
     ]
    }
   ],
   "source": [
    "t1 = time()\n",
    "trainer.train()\n",
    "print(f\"{time()-t1:2f} s to train\")\n",
    "del t1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
